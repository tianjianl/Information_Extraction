{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HMM Tutorial\n",
        "### Wed Jan 25 | Tianjian Li | tli104@jhu.edu\n",
        "\n",
        "This notebook contains three things:\n",
        "\n",
        "a) An step by step implementation and explaination of the **Foward-Backward Algorithm** in Python\n",
        "\n",
        "b) An step by step implementation of the **Baum-Welch Algorithm** in Python\n"
      ],
      "metadata": {
        "id": "uofiuAQPPmFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Forward-Backward Algorithm\n",
        "\n",
        "Hidden Random Variables $Z = \\{z_1, z_2, ..., z_n\\}, z_i \\in \\{1,2,...,m\\}$\n",
        " \n",
        "Observed Variables $X = {x_1, ..., x_n}$\n",
        "\n",
        "The foward-backward algorithm computes the probability of $p(z_k|x)$.\n",
        "\n",
        "The foward algorithm computes $p(z_k ,x_{1:k})$\n",
        "\n",
        "The backward algorithm computes $ p(x_{k+1:n} | z_k)$\n",
        "\n",
        "$p(z_k | x) \\propto p(z_k, x) = p(x_{k+1:n}|z_k, x_{1:k}) \\cdot p(z_k, x_{1:k})$\n"
      ],
      "metadata": {
        "id": "qvVffLv1E5vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def forward(transition, emission, initial, X):\n",
        "\n",
        "  \"\"\"\n",
        "  The forward algorithm\n",
        "  \n",
        "  Parameters:\n",
        "  emission: the emission matrix defining p(x_t|z_t) = emission[z_t, x_t]\n",
        "  transition: the transition matrix defining p(z_t|z_{t-1}) = transition[z_{t-1}, z_t]\n",
        "  initial: the initial probability defining p(z_1)\n",
        "  X: the observations x_1, .... , x_T\n",
        "\n",
        "  Returns:\n",
        "  A matrix alpha such that\n",
        "    alpha[z_t, t] = p(z_t, x_{1:t})\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # m is the number of states\n",
        "  m = transition.shape[0]\n",
        "  # n is the sequence length\n",
        "  T = len(X)\n",
        "\n",
        "  alpha = np.zeros((m, T))\n",
        "  alpha_debug = np.zeros((m, T))\n",
        "  \n",
        "  #initialization, the more efficient version\n",
        "  alpha[:,0] = initial * emission[:, X[0]]\n",
        "\n",
        "  #and the more understandable version\n",
        "  for z_0 in range(m):\n",
        "    alpha_debug[z_0, 0] = initial[z_0] * emission[z_0, X[0]]\n",
        "  \n",
        "  for t in range(1, T):\n",
        "    for z_t in range(m):\n",
        "      # For efficient computation, use\n",
        "      alpha[z_t, t] = np.dot(alpha[:, t-1], transition[:, z_t]) * emission[z_t, X[t]]\n",
        "      \n",
        "      # The more understandable version if we set z_t_prev = z_{t-1}\n",
        "      for z_t_prev in range(m):\n",
        "        alpha_debug[z_t, t] += alpha_debug[z_t_prev, t-1] * transition[z_t_prev, z_t] * emission[z_t, X[t]]\n",
        "  \n",
        "  assert alpha.all() == alpha_debug.all(), \"mismatch in alpha\"\n",
        "  return alpha"
      ],
      "metadata": {
        "id": "dwYzwqREPqEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is a sanity check \n",
        "\n",
        "emis = np.array([[0.1, 0.9],\n",
        "                [0.4, 0.6]])\n",
        "\n",
        "trans = np.array([[0.7, 0.3],\n",
        "                  [0.4, 0.6]])\n",
        "\n",
        "init = np.array([0.5, 0.5])\n",
        "\n",
        "obs = np.array([0, 1, 1, 0, 1])\n",
        "\n",
        "_ = forward(trans, emis, init, obs)"
      ],
      "metadata": {
        "id": "nbEbsdduRafn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(transition, emission, X):\n",
        "\n",
        "  \"\"\"\n",
        "  The backward algorithm\n",
        "  \n",
        "  Parameters:\n",
        "  emission: the emission matrix defining p(x_t|z_t) = emission[z_t, x_t]\n",
        "  transition: the transition matrix defining p(z_t|z_{t-1}) = transition[z_{t-1}, z_t]\n",
        "  X: the observations x_1, .... , x_T\n",
        "\n",
        "  Returns:\n",
        "  A matrix beta such that\n",
        "    beta[z_t, t] = p(x_{t:n} | z_t)\n",
        "  \"\"\"\n",
        "  # m is the number of states\n",
        "  m = transition.shape[0]\n",
        "  # n is the sequence length\n",
        "  T = len(X)\n",
        "\n",
        "  beta = np.zeros((m, T))\n",
        "  beta_debug = np.zeros((m, T))\n",
        "  \n",
        "  #initialization, the more efficient version\n",
        "  beta[:,T-1] = 1\n",
        "  beta_debug[:, T-1] = 1\n",
        "\n",
        "  for t in range(T-2, -1, -1):\n",
        "    for z_t in range(m):\n",
        "      # Efficient Computation\n",
        "      beta[z_t, t] = np.dot(beta[:, t+1] * emission[:, X[t+1]], transition[z_t, :])\n",
        "\n",
        "      # More understandably, we denote z_{t+1} as z_t_next\n",
        "      for z_t_next in range(m):\n",
        "        beta_debug[z_t, t] += beta_debug[z_t_next, t+1] * emission[z_t_next, X[t+1]] * transition[z_t, z_t_next]\n",
        "        \n",
        "  assert beta.all() == beta_debug.all()\n",
        "  return beta"
      ],
      "metadata": {
        "id": "cdw2TQVtR4fZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again we check if our implementation is correct"
      ],
      "metadata": {
        "id": "blfmLTwDVo_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emis = np.array([[0.1, 0.9],\n",
        "                [0.4, 0.6]])\n",
        "\n",
        "trans = np.array([[0.7, 0.3],\n",
        "                  [0.4, 0.6]])\n",
        "\n",
        "init = np.array([0.5, 0.5])\n",
        "\n",
        "obs = np.array([0, 1, 1, 0, 1])\n",
        "\n",
        "_ = backward(trans, emis, obs)"
      ],
      "metadata": {
        "id": "_OTwJ1BsVoG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that \n",
        "\n",
        "$p(z_k | x) \\propto p(z_k, x) = p(x_{k+1:n}|z_k, x_{1:k}) \\cdot p(z_k, x_{1:k})$\n",
        "\n",
        "We combine our forward algorithm and backward algorithm and normalize to get the final probabilty vector of $p(z_k, x)$"
      ],
      "metadata": {
        "id": "D7KQNobPV8U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_backward(transition, emission, initial, X):\n",
        "  alpha = forward(transition, emission, initial, X)\n",
        "  beta = backward(transition, emission, X)\n",
        "  \n",
        "  probs = alpha * beta \n",
        "  # This is a matrix of m * T, inidicating the unmarginalized probability of p(z_k, X)\n",
        "  probs /= np.sum(probs, axis=0)\n",
        "  return alpha, beta, probs"
      ],
      "metadata": {
        "id": "zD7U4MtFXkz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emis = np.array([[0.1, 0.9],\n",
        "                [0.4, 0.6]])\n",
        "\n",
        "trans = np.array([[0.7, 0.3],\n",
        "                  [0.4, 0.6]])\n",
        "\n",
        "init = np.array([0.5, 0.5])\n",
        "\n",
        "obs = np.array([0, 1, 1, 0, 1])\n",
        "\n",
        "_, _, p = forward_backward(trans, emis, init, obs)\n",
        "p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCBRiOt-WxW-",
        "outputId": "44e6026c-fee7-45a6-b5b5-cc63f373478d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22015293, 0.56429141, 0.5793029 , 0.29596097, 0.58221138],\n",
              "       [0.77984707, 0.43570859, 0.4206971 , 0.70403903, 0.41778862]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Baum-Welch Algorithm\n",
        "\n",
        "The Baum-Welch Algorithm is an example of the **Expectation-Maximization algorithm** for finding the paramteres of an Hidden Markov Model. The following function is a single iteration of the Baum-Welch Algorithm."
      ],
      "metadata": {
        "id": "xwYFBFQ-YV85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def baum_welch(observations, states, start_prob, trans_prob, emit_prob, forward_backward):\n",
        "    \n",
        "    \"\"\"\n",
        "    Implements one pass of the Baum-Welch Algorithm for estimating the parameters of a Hidden Markov Model.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize variables\n",
        "    T = len(observations)\n",
        "    m = len(states)\n",
        "    A = np.zeros((m, m))\n",
        "    B = np.zeros((m, T))\n",
        "    alpha, beta, prob = forward_backward(observations, states, start_prob, trans_prob, emit_prob)\n",
        "    # Compute the expected state transition counts\n",
        "    for t in range(T - 1):\n",
        "        for i in range(m):\n",
        "            for j in range(m):\n",
        "                A[i][j] += alpha[t][i] * trans_prob[i][j] * emit_prob[j][observations[t + 1]] * beta[t + 1][j] / prob\n",
        "    \n",
        "    # Compute the expected observation symbol counts\n",
        "    for t in range(T):\n",
        "        for i in range(m):\n",
        "            B[i][observations[t]] += alpha[t][i] * beta[t][i] / prob\n",
        "    \n",
        "    # Normalize the expected state transition and observation symbol counts\n",
        "    for i in range(m):\n",
        "        A[i] /= np.sum(A[i])\n",
        "        B[i] /= np.sum(B[i])\n",
        "    \n",
        "    # Return the estimated parameters\n",
        "    return A, B"
      ],
      "metadata": {
        "id": "kUiscZhrYdXd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}